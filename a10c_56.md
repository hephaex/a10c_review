# ARM10C 56주차 후기
##### 일시 : 2014.05.31 (56주차)
##### 모임명 : NAVER개발자커뮤니티지원_IAMROOT.ORG_10차ARM-C
##### 장소 : 토즈 타워점
##### 장소지원 : NAVER 개발자 커뮤니티 지원 프로그램
##### 참여인원 :  5명
## 스터디 진도 : 
 - 지난 스터디에 이어서 mem_init()을 계속 분석합니다.
 - start_kernel()-> mm_init()->kmem_cache_init()->create_boot_cache() 분석중


## slub.c::kmem_cache_init()
// ARM10C 20140419
// &boot_kmem_cache_node, "kmem_cache_node", sizeof(struct kmem_cache_node): 44 byte,
// SLAB_HWCACHE_ALIGN: 0x00002000UL

```
create_boot_cache(kmem_cache_node, "kmem_cache_node", \
	sizeof(struct kmem_cache_node), SLAB_HWCACHE_ALIGN);
```

## slab_common.c::create_boot_cache()
// ARM10C 20140419
// &boot_kmem_cache_node, "kmem_cache_node", sizeof(struct kmem_cache_node): 44 byte,
// SLAB_HWCACHE_ALIGN: 0x00002000UL

```
void __init create_boot_cache(struct kmem_cache *s, const char *name, size_t size,
		unsigned long flags)
{
	int err;

	// s->name: boot_kmem_cache_node.name: NULL
	s->name = name;
	// s->name: boot_kmem_cache_node.name: "kmem_cache_node"

	// s->size: boot_kmem_cache_node.size: 0
	// s->object_size: boot_kmem_cache_node.object_size: 0
	s->size = s->object_size = size;
	// s->size: boot_kmem_cache_node.size: 44
	// s->object_size: boot_kmem_cache_node.object_size: 44
	
	// flags: SLAB_HWCACHE_ALIGN: 0x00002000UL, ARCH_KMALLOC_MINALIGN: 64, size: 44
	// s->align: boot_kmem_cache_node.align: 0
	s->align = calculate_alignment(flags, ARCH_KMALLOC_MINALIGN, size);
	// s->align: boot_kmem_cache_node.align: 64
	
	// s: &boot_kmem_cache_node, flags: SLAB_HWCACHE_ALIGN: 0x00002000UL
	err = __kmem_cache_create(s, flags);
```

## slub.c::__kmem_cache_create()
// ARM10C 20140419
// s: &boot_kmem_cache_node,
// flags: SLAB_HWCACHE_ALIGN: 0x00002000UL

```
int __kmem_cache_create(struct kmem_cache *s, unsigned long flags)
{
	int err;

	// s: &boot_kmem_cache_node, flags: SLAB_HWCACHE_ALIGN: 0x00002000UL
	err = kmem_cache_open(s, flags);
```

## slub.c::kmem_cache_open()
// ARM10C 20140419
// s: &boot_kmem_cache_node,
// flags: SLAB_HWCACHE_ALIGN: 0x00002000UL

```
static int kmem_cache_open(struct kmem_cache *s, unsigned long flags)
{
	// s->size: boot_kmem_cache_node.size: 44, flags: SLAB_HWCACHE_ALIGN: 0x00002000UL,
	// s->name: boot_kmem_cache_node.name: "kmem_cache_node, s->ctor: boot_kmem_cache_node.ctor: NULL
	s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
	// s->flags: boot_kmem_cache_node.flags: SLAB_HWCACHE_ALIGN: 0x00002000UL

	// s->reserved: boot_kmem_cache_node.reserved: 0
	s->reserved = 0;
	// s->reserved: boot_kmem_cache_node.reserved: 0

	// need_reserve_slab_rcu: 0 , s->flags: boot_kmem_cache_node.flags: SLAB_HWCACHE_ALIGN
	if (need_reserve_slab_rcu && (s->flags & SLAB_DESTROY_BY_RCU))
		s->reserved = sizeof(struct rcu_head);

	// s: &boot_kmem_cache_node, -1, calculate_sizes(&boot_kmem_cache_node, -1): 1
	if (!calculate_sizes(s, -1))
		goto error;

	// disable_higher_order_debug: 0
	if (disable_higher_order_debug) {
		/*
		 * Disable debugging flags that store metadata if the min slab
		 * order increased.
		 */
		if (get_order(s->size) > get_order(s->object_size)) {
			s->flags &= ~DEBUG_METADATA_FLAGS;
			s->offset = 0;
			if (!calculate_sizes(s, -1))
				goto error;
		}
	}

// CONFIG_HAVE_CMPXCHG_DOUBLE=n, CONFIG_HAVE_ALIGNED_STRUCT_PAGE=n
#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \
    defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)
	if (system_has_cmpxchg_double() && (s->flags & SLAB_DEBUG_FLAGS) == 0)
		/* Enable fast mode */
		s->flags |= __CMPXCHG_DOUBLE;
#endif

	/*
	 * The larger the object size is, the more pages we want on the partial
	 * list to avoid pounding the page allocator excessively.
	 */
	// s->size: boot_kmem_cache_node.size: 64, ilog2(64): 6
	// s: &boot_kmem_cache_node, 3
	set_min_partial(s, ilog2(s->size) / 2);
	// boot_kmem_cache_node.min_partial: 5

	/*
	 * cpu_partial determined the maximum number of objects kept in the
	 * per cpu partial lists of a processor.
	 *
	 * Per cpu partial lists mainly contain slabs that just have one
	 * object freed. If they are used for allocation then they can be
	 * filled up again with minimal effort. The slab will never hit the
	 * per node partial lists and therefore no locking will be required.
	 *
	 * This setting also determines
	 *
	 * A) The number of objects from per cpu partial slabs dumped to the
	 *    per node list when we reach the limit.
	 * B) The number of objects in cpu partial slabs to extract from the
	 *    per node list when we run out of per cpu objects. We only fetch
	 *    50% to keep some capacity around for frees.
	 */

	// s: &boot_kmem_cache_node, kmem_cache_has_cpu_partial(&boot_kmem_cache_node): 1
	// s->size: boot_kmem_cache_node.size: 64, PAGE_SIZE: 0x1000
	if (!kmem_cache_has_cpu_partial(s))
		s->cpu_partial = 0;
	else if (s->size >= PAGE_SIZE)
		s->cpu_partial = 2;
	else if (s->size >= 1024)
		s->cpu_partial = 6;
	else if (s->size >= 256)
		s->cpu_partial = 13;
	else
		// s->cpu_partial: boot_kmem_cache_node.cpu_partial: 0
		s->cpu_partial = 30;
		// boot_kmem_cache_node.cpu_partial: 30

#ifdef CONFIG_NUMA // CONFIG_NUMA=n
	s->remote_node_defrag_ratio = 1000;
#endif
	// s: &boot_kmem_cache_node, init_kmem_cache_nodes(&boot_kmem_cache_node): 1
	if (!init_kmem_cache_nodes(s))
		goto error;
```

## slub.c::init_kmem_cache_nodes()
// s: &boot_kmem_cache_node, 
// return: init_kmem_cache_nodes(&boot_kmem_cache_node): 1

```slub.c::init_kmem_cache_nodes()
static int init_kmem_cache_nodes(struct kmem_cache *s)
{
	int node;

	// N_NORMAL_MEMORY: 2
	for_each_node_state(node, N_NORMAL_MEMORY) {
	// for ( (node) = 0; (node) == 0; (node) = 1)

		struct kmem_cache_node *n;

		// slab_state: DOWN: 1
		if (slab_state == DOWN) {
			// node: 0
			early_kmem_cache_node_alloc(node);
			continue;
		}
```

## slub.c::early_kmem_cache_node_alloc()
// ARM10C 20140426
// node: 0

```
static void early_kmem_cache_node_alloc(int node)
{
	struct page *page;
	struct kmem_cache_node *n;

	// kmem_cache_node->size: boot_kmem_cache_node.size: 64, sizeof(struct kmem_cache_node): 44 bytes
	BUG_ON(kmem_cache_node->size < sizeof(struct kmem_cache_node));

	// kmem_cache_node: &boot_kmem_cache_node, GFP_NOWAIT: 0, node: 0
	page = new_slab(kmem_cache_node, GFP_NOWAIT, node);
```

## slub.c::new_slab()
// ARM10C 20140426
// kmem_cache_node: &boot_kmem_cache_node, GFP_NOWAIT: 0, node: 0

```
static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
{
	struct page *page;
	void *start;
	void *last;
	void *p;
	int order;

	// flags: GFP_NOWAIT: 0, GFP_SLAB_BUG_MASK: 0xfe000005
	BUG_ON(flags & GFP_SLAB_BUG_MASK);

	// s: &boot_kmem_cache_node, flags: GFP_NOWAIT: 0, node: 0
	// GFP_RECLAIM_MASK: 0x13ef0, GFP_CONSTRAINT_MASK: 0x60000
	page = allocate_slab(s,
		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
	// page: migratetype이 MIGRATE_UNMOVABLE인 page

	if (!page)
		goto out;

	// page: migratetype이 MIGRATE_UNMOVABLE인 page
	// compound_order(page): 0
	order = compound_order(page);
	// free_pages_check에서 page->flags의 NR_PAGEFLAGS 만큼의 하위 비트를 전부 지워줌
	// order: 0

```

## mm.h::compound_order()
// ARM10C 20140524
// page: migratetype이 MIGRATE_UNMOVABLE인 page

```
static inline int compound_order(struct page *page)
{
	// PageHead(page): 0
	if (!PageHead(page))
		return 0;
	return (unsigned long)page[1].lru.prev;
}
```

order = compound_order(page);
// order: 0
// free_pages_check에서 page->flags의 NR_PAGEFLAGS 만큼의 하위 비트를 전부 지워줌
* compound가 접착이라는 의미이므로 page가 붙여서 할당 받았는지 여부를 나타내는 것으로 추정된다.
* 우리는 작은 것을 받았으므로 0일것이다. 하지만 의미는 분명하게 확인해 보자.
* free_pages_check에서 버디를 활성화하기 전에 모두 0으로 초기화 하고 왔다
* page->flags의 NR_PAGEFLAGS 만큼의 하위 비트를 전부 지워줌.

## 	slub.c::new_slab()

```
inc_slabs_node(s, page_to_nid(page), page->objects);
// s: &boot_kmem_cache_node, page:
//
static inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)
{
    // s: &boot_kmem_cache_node, node:0
	// get_node(&boot_kmem_cache_node,0) : (&boot_kmem_cache_node)->node[0]
    struct kmem_cache_node *n = get_node(s, node);

```


## get_node(s,node)

```
static inline sector_t *get_node(struct dm_table *t,
				 unsigned int l, unsigned int n)
{
    // s->node : (&boot_kmem_cache_node)->node[0]
    return t->index[l] + (n * KEYS_PER_NODE);
	// return: &(&boot_kmem_cache_node)->node[0]: NULL
}
```

## 	inc_slabs_node(s, page_to_nid(page), page->objects);
```
// s: &boot_kmem_cache_node, page:
//
static inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)
{
    // s: &boot_kmem_cache_node, node:0
	// get_node(&boot_kmem_cache_node,0) : (&boot_kmem_cache_node)->node[0]
    struct kmem_cache_node *n = get_node(s, node);
	// n: &(&boot_kmem_cache_node)->node[0]
	/*
	 * May be called early in order to allocate a slab for the
	 * kmem_cache_node structure. Solve the chicken-egg
	 * dilemma by deferring the increment of the count during
	 * bootstrap (see early_kmem_cache_node_alloc).
	 */
	// n : &(&boot_kmem_cache_node)->node[0]
	if (likely(n)) {
		atomic_long_inc(&n->nr_slabs);
		atomic_long_add(objects, &n->total_objects);
	}
}
```
여기서 if(likely(n))에서 n의 값을 확인해 보자.
n은 n : &(&boot_kmem_cache_node)->node[0]이므로
NULL이다.
inc_slabs_node 는 나중에 kmem_cache_node가 완성된 이후면 증가될것으로 예상한다. 

## slub.c::new_slab()
```
    inc_slabs_node(s, page_to_nid(page), page->objects);
    // s: &boot_kmem_cache_node
	// order : 0
    memcg_bind_pages(s, order);
	// null function

    // page : migratetype이 MIGRATE_UNMOVEABLE인 page
	// s : &boot_kmem_cache_node
	page->slab_cache = s;
	// page->slab_cache : &boot_kmem_cache_node

   	__SetPageSlab(page);
```

## page-flags.h:: __SetPageSlab(page)
매크로 이다.

```
__PAGEFLAG(Slab, slab)
__SetPageSlab(strunct page *page) \
 {__set_bits(PG_slab, &page->flags);}
```
# define __SETPAGEFLAG(Slab,slab); \
static inline void _SetPageSlab(struct page *page) \
 { __set_bits(PG_slab, &page->flags);
```
// PG_slab : enum 7
page->flags의 7번째 bit를 set한다.

## slub.c::new_slab()
```
	if (page->pfmemalloc)
	// page->pfmemalloc : 0
		SetPageSlabPfmemalloc(page);

    // page : &boot_kmem_cache_node
	start = page_address(page);
    // start : UNMOVEABLE인 page의 virtual address

	// s : &boot_kmem_cache_node->flags
    //   : &boot_kmem_cache_node.flags
	// SLAB_POISON :  0x00000800UL
    if (unlikely(s->flags & SLAB_POISON))
	// false

    // start : UNMOVEABLE인 page의 가상 주소
   	last = start;
    // last : UNMOVEABLE인 page의 가상 주소
	
    // p : *
	// s : &boot_kmem_cahce_node
	// start : UNMOVEABLE인 page의 가상 주소
	// page->objects : 64
	// 
    //	#define for_each_object(__p, __s, __addr, __objects) \
	// for (__p = (__addr); __p < (__addr) + (__objects) * (__s)->size;\
	//		__p += (__s)->size)
	// 
    // for (p = (UNMOVEABLE인 page의 가상 주소); \
	//          p < (UNMOVEABLE인 page의 가상 주소) + (__objects) * (__s)->size; \
	//          p += (&boot_kmem_cache_node)->size)
    // 
    // for (p = (UNMOVEABLE인 page의 가상 주소); \
	//          p < (UNMOVEABLE인 page의 가상 주소) + (64) * (&boot_kmem_cahce_node)->size; \
	//          p += (&boot_kmem_cache_node)->size)
	
    for_each_object(p, s, start, page->objects)
    
    // for( ...) {

        // s : &boot_kmem_cahce_node
		// page : UNMOVEABLE인 page의 가상 주소
		// last : UNMOVEABLE인 page의 가상 주소
		setup_object(s, page, last);
		set_freepointer(s, last, p);
		last = p;
	}

```
* page->pfmemalloc : page_alloc.c : 2887에서 0으로 초기화 했다.
* page를 migratetype이 MI GRATE_UNMOVEABLE 로 쓰는 것에서
UNMOVEABLE로 쓰기로 하자. 

page_address(page) :

## setup_objects(s page last) 

```
static void setup_object(struct kmem_cache *s, struct page *page,
				void *object)
{
    setup_object_debug(s, page, object);

```

## setup_object_debug()

```
static void setup_object_debug(struct kmem_cache *s, struct page *page,
								void *object)
{
    // s->flags :
	// SLAB_STORE_USER: 0x000100000
	// SLAB_RED_ZONE:   0x000000400
	// __OBJECT_POISON: 0x800000000
	// s->flags & 0x80010400
    if (!(s->flags & (SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON)))
	    // true
		return;

	init_object(s, object, SLUB_RED_INACTIVE);
	init_tracking(s, object);
}
```

if문에서 참값으로 return한다.

## setup_objects(s page last) 

```
static void setup_object(struct kmem_cache *s, struct page *page,
				void *object)
{
    setup_object_debug(s, page, object);
    // s->ctor : boot_kmem_caceh_node.ctor: NULL
    if (unlikely(s->ctor))
    // false
		s->ctor(object);
}
```

## slub.c::newslab()
```
	for_each_object(p, s, start, page->objects) {
		setup_object(s, page, last);
        // s: &boot_kmem_cache_node
		// last : UNMOVERABLE인 page의 가상 주소
		// p: UNMOVERABLE인 page의 가상 주소
        set_freepointer(s, last, p);
		
		last = p;
	}
```

## slub.c::set_freepoint(s,page,last)

```
static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
{
    // object : last : UNMOVERABLE인 page의 가상 주소
	// s->offset : &boot_kmem_cache_node->offset : 0
	// fp : 
	*(void **)(object + s->offset) = fp;
	// UNMOVERABLE인 page의 가상 주소
}
```
*(void **)(object + s->offset) = fp
object + s->offset를 더한 주소에 fp를 넣는다.
즉 next를 가리키는 구조가 된다.
|0x10001000|data|0x10001040|data|0x10000080|data|...|null|data|화
한 페이지당 60Byte * 60 = 3840 Byte, 사용가능.

last에 p주소를 매핑함.

for_each_object가 하는일:
다음 object의 시작주소를 이전 object의 내부에 저장을 하는데.
그 위치는 s->offset에 의해 결정되어 저장됨.

## slub.c::new_slab()
```
	for_each_object(p, s, start, page->objects) {...}
    // for_each_object가 하는일:
	// 다음 object의 시작주소를 이전 object의 내부에 저장을 하는데.
	// 그 위치는 s->offset에 의해 결정되어 저장됨.

    // s: &boot_kmem_cache_node:, page : UNMOVEABLE인 page
	// last: UNMOVEABLE인 page의 가상 주소 + 0x1000 - 64
	setup_object(s, page, last);
	set_freepointer(s, last, NULL);

    // 마지막 object의 freepoint는 null 로 초기화

    // page->freelist: UNMOVEABLE인 page의 가상 주소.
	// page의 freelist멤버는 slab의 oject의 시작 주소를 가리킴.
	page->freelist = start;
	// page->objects : 64
	page->inuse = page->objects;
	// page->inuse : 64
	//
	// page의 object 멤버는 slab의 object의 총 갯수
	// page의 inuse 멤버는 slab의 free상태인 object의 총 갯수
	page->frozen = 1;
	// page->frozen : 1
out:
	return page;
```

* net_slab()이 한일
* migratype이 MIGRATE_UNMOVEABLE인 page 를 할당 받았고,
* page->flags의 NR_PAGEFLAGS를 지우고 (PG_slab) bit를 set
* slab 의 object들의 freepoint를 매핑 함.
* kmem_cache_node가 완성된 이후에 nr_slabs, total_objects가 증가할 것으로 예상됨
* page 맴버를 셋팅함
* page->freelist : UNMOVEABLE인 page의 가상주소, page->inuse:64
* page->frozen : 1로 설정함

## slub.c::early_kmem_cache_node_alloc()
```
// node: 0
static void early_kmem_cache_node_alloc(int node)
{
...
	page = new_slab(kmem_cache_node, GFP_NOWAIT, node);

    // page: UNMOVABLE인 page
	BUG_ON(!page);
	// page: UNMOVABLE인 page, node: 0
	// if: false
	if (page_to_nid(page) != node) {
		printk(KERN_ERR "SLUB: Unable to allocate memory from "
				"node %d\n", node);
		printk(KERN_ERR "SLUB: Allocating a useless per node structure "
				"in order to be able to continue\n");
	}
	
    // page->freelist: UNMOVABLE인 page 의 object시작 가상주소
	n = page->freelist;
	// n: page->freelist: UNMOVABLE인 page 의 object시작 가상주소
	BUG_ON(!n);
	
	// kmem_cache_node : &boot_kmem_cache_node
	// n: page->freelist: UNMOVABLE인 page 의 object시작 가상주소
	page->freelist = get_freepointer(kmem_cache_node, n);
	// 할당받았던 slub_object을 처음 주소
```

## get_freepointer
// kmem_cache_node : &boot_kmem_cache_node
// n: page->freelist: UNMOVABLE인 page 의 object시작 가상주소
page->freelist = get_freepointer(kmem_cache_node, n);


```
static inline void *get_freepointer(struct kmem_cache *s, void *object)
{
	return *(void **)(object + s->offset);
}
```

gett_freepointer가 하는일
* 다음 object의 주소를 가져옴. 
* // 할당받았던 slub_object을 처음 주소에 값을 가지고 오기 때문에.
* // 다음 위치(next)주고가 된다.
* 다음 freepointer를 가리킨다.

## early_kmem_cache_node_alloc()

```
...
    // page->inuse : 64 
	page->inuse = 1;
	// page의 inuse맴버는 현재 사용하고 있는 object수를 의미함
	// (64)에서 1로 바뀜. (사용중인 slab 수)
	// page->frozen : 1
	page->frozen = 0;
	// page->frozen : 0
	
	// kmem_cache_node->node : (&boot_kmem_cache_node)->node[0]: NULL
	// n: UNMOVEABLE인 page의 시작 object의 가상 주소
	kmem_cache_node->node[node] = n;
	// kmem_cache_node->node[0]: 
#ifdef CONFIG_SLUB_DEBUG // y
 	// kmem_cache_node->node[0]:
	// #define SLUB_RED_ACTIVE		0xcc
	init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);
```

## slub.c::init_object()

// kmem_cache_node->node[0]:
// init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);

```
static void init_object(struct kmem_cache *s, void *object, u8 val)
{
	u8 *p = object;

	if (s->flags & __OBJECT_POISON) {
		memset(p, POISON_FREE, s->object_size - 1);
		p[s->object_size - 1] = POISON_END;
	}

	if (s->flags & SLAB_RED_ZONE)
		memset(p + s->object_size, val, s->inuse - s->object_size);
}
```

## slub.c::early_kmem_cache_node_alloc()

```
	init_tracking(kmem_cache_node, n);
```

## init_tracking()
```
static void init_tracking(struct kmem_cache *s, void *object)
{
    // s->flags: (&boot_kmem_cache_node)->flags : 
	if (!(s->flags & SLAB_STORE_USER))
		return;

	set_track(s, object, TRACK_FREE, 0UL);
	set_track(s, object, TRACK_ALLOC, 0UL);
}
```
if문에서 리턴함

## slub.c::early_kmem_cache_node_alloc()

```
    // n: UNMOVEABLE인 page의 object의 시작 virtual address
    init_kmem_cache_node(n);
```
## init_kmem_cache_node

// n: UNMOVEABLE의 page의 object의 시작 virtual address
여기서 할당 받았던 slab을 kmem_cache_node로 초기화를 여기서 하는 과정이다.

```
static void
init_kmem_cache_node(struct kmem_cache_node *n)
{
    // n: UNMOVEABLE의 page의 object의 시작 virtual address
    n->nr_partial = 0;
	// n->nr_partial: 0
	
	spin_lock_init(&n->list_lock);
	// kmem_cache_node에 spinlock 맴버 변수를 초기화
	
    // n->partial: 0
    INIT_LIST_HEAD(&n->partial);
	// n->partial 리스트 초기화
#ifdef CONFIG_SLUB_DEBUG // y
	atomic_long_set(&n->nr_slabs, 0);
	// n->nr_slabs:0
	atomic_long_set(&n->total_objects, 0);
	// n->total_objects: 0
	INIT_LIST_HEAD(&n->full);
	//
#endif
}
```

참조
```
struct kmem_cache_node {
	spinlock_t list_lock;

	unsigned long nr_partial;
	struct list_head partial;
	atomic_long_t nr_slabs;
	atomic_long_t total_objects;
	struct list_head full;
};
```
## slub.c::early_kmem_cache_node_alloc()
```
    init_kmem_cache_node(n);
	// 할당받은 slab object를 kmem_cache_node로 사용하고,
	// kmem_cache_node의 맴버 필들를 초기화함.

    // kmem_cache_node: &boot_kmem_cache_node, node: 0, page->object:64
	inc_slabs_node(kmem_cache_node, node, page->objects);a
```

## slub.c::inc_slabs_node()
이번에는 n이 값이 있으므로 likely를 실햄함. 
```
static inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)
{
	struct kmem_cache_node *n = get_node(s, node);

	/*
	 * May be called early in order to allocate a slab for the
	 * kmem_cache_node structure. Solve the chicken-egg
	 * dilemma by deferring the increment of the count during
	 * bootstrap (see early_kmem_cache_node_alloc).
	 */
	// n
	if (likely(n)) {
	    // n->slabs: 0, 
		atomic_long_inc(&n->nr_slabs);
		// n->slabs: 1
	    // object: 64, n->total_objects: 0
		atomic_long_add(objects, &n->total_objects);
		// n->total_objects: 64
	}
	// kmem_cache_node가 완성되었기 때문에 예상대로 증가했음. 
}
```
## slub.c::early_kmem_cache_node_alloc()
```
    // kmem_cache_node: &boot_kmem_cache_node, node: 0, page->object:64
	inc_slabs_node(kmem_cache_node, node, page->objects);a
    // n->slabs: 1, n->total_objects: 64로 설정함
	// n:  
	// page:  
	// 	15: enum DEACTIVATE_TO_HEAD,	/* Cpu slab was moved to the head of partials */
    add_partial(n, page, DEACTIVATE_TO_HEAD);
}
```

## add_partial()
n: UNMOVEABLE인 page의 object의 시작 virtual address
```
static inline void add_partial(struct kmem_cache_node *n,
				struct page *page, int tail)
{
    // n->nr-partial: 0
	n->nr_partial++;
    // n->nr-partial: 0

    // tail : 14, DEACTIVATE_TO_TAIL : 16
    if (tail == DEACTIVATE_TO_TAIL)
		list_add_tail(&page->lru, &n->partial);
	else
	    // page->lru: (UNMOVEABLE인 page)->lru, n->partial : NULL
		list_add(&page->lru, &n->partial);
		// n->partial에 &page->lru 리스트가 추가됨.
}
```

## slub.c::early_kmem_cache_node_alloc()
```
    // kmem_cache_node: &boot_kmem_cache_node, node: 0, page->object:64
	inc_slabs_node(kmem_cache_node, node, page->objects);a
    // n->slabs: 1, n->total_objects: 64로 설정함
	// n:  
	// page:  
	// 	15: enum DEACTIVATE_TO_HEAD,	/* Cpu slab was moved to the head of partials */
    add_partial(n, page, DEACTIVATE_TO_HEAD);
	// kmem_cache_node의 partial 맴버의 현재 page의 lru 리스트를 추가함
}
```

## slub.c::init_kmem_cache_node()
```
// s: &boot_kmem_cache_node
static int init_kmem_cache_nodes(struct kmem_cache *s)
{
	int node;

	// N_NORMAL_MEMORY: 2
	for_each_node_state(node, N_NORMAL_MEMORY) {
	// for ( (node) = 0; (node) == 0; (node) = 1)

		struct kmem_cache_node *n;

		// slab_state: DOWN: 1
		if (slab_state == DOWN) {
			// node: 0
			early_kmem_cache_node_alloc(node);
			// 
			continue;
		}
```
* new_slab이 한일
 - migratetype이 MIGRATE_UNMOVEABLE인 page를 할당 받음.
 - page->flags에 7(PG_slab) bit를 초기화
 - slab의 objects들을 freepointer를 맵핑함.
 - page->freelist: UNMOVEABLE인 page의 object의 시작 가상 주소 + 64
* init_kmem_cache_node()
 - n->nr_partial : 0
 - n->list_lock : spinglock 초기화함
 - n->partial : 리스트 초기화
 - n->nr_slabs: 0
 - n->slabs: 1,
 - kmem_cache_node->total_objects: 64
 - kmem_cache_node->full: 리스트 초기화

* kmem_cache_alloc_node로 할당 받고,
* free_kmem_cache_nodes로 프리시킨다.

## slub.c::kmem_cache_open()
```
	if (!init_kmem_cache_nodes(s))
		goto error;

    if (alloc_kmem_cache_cpus(s))
		return 0;
```

## slub.c::alloc_kmem_cache_cpus()
```
static inline int alloc_kmem_cache_cpus(struct kmem_cache *s)
{
    // PERCPU_DYNAMIC_EARLY_SIZE: 0x3000, KMALLOC_SHIFT_HIGH: 13
	// sizeof(struct kmem_cache_cpu): 16 bytes
	BUILD_BUG_ON(PERCPU_DYNAMIC_EARLY_SIZE <
			KMALLOC_SHIFT_HIGH * sizeof(struct kmem_cache_cpu));

	/*
	 * Must align to double word boundary for the double cmpxchg
	 * instructions to work; see __pcpu_double_call_return_bool().
	 */
	// s->
	s->cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu),
				     2 * sizeof(void *));

	if (!s->cpu_slab)
		return 0;

	init_kmem_cache_cpus(s);

	return 1;
}
```

## percpu.c:__alloc_percpu()
```
void __percpu *__alloc_percpu(size_t size, size_t align)
{
	return pcpu_alloc(size, align, false);
}
```

## percpu.c::*pcpu_alloc()
```
static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved)
{
	static int warn_limit = 10;
	struct pcpu_chunk *chunk;
	const char *err;
	int slot, off, new_alloc;
	unsigned long flags;
	void __percpu *ptr;

	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE)) {
	// size: 16, PCPU_MIN_UNIT_SIZE: 0x8000, align: 8, PAGE_SIZE: 0x1000
	// false
		WARN(true, "illegal size (%zu) or align (%zu) for "
		     "percpu allocation\n", size, align);
		return NULL;
	}


	mutex_lock(&pcpu_alloc_mutex);
    // &pcpu_alloc_mutex) 에 잠금을 걸다.
	
	spin_lock_irqsave(&pcpu_lock, flags);
	// pcpu_lock을 이용해서 spinlock을 걸고 cpsr을 flags에 저장한다.


	/* serve reserved allocations from the reserved chunk if available */
	// reserved: false, pcpu_reserved ...
	if (reserved && pcpu_reserved_chunk) {
	// if: false

    ...

restart:
	/* search through normal chunks */
	for (slot = pcpu_size_to_slot(size); slot < pcpu_nr_slots; slot++) {
	// pcpu_size_to_slot: size:16
		list_for_each_entry(chunk, &pcpu_slot[slot], list) {
			if (size > chunk->contig_hint)
				continue;
```

## percpu.c::pcpu_size_to_slot()
```
// pcpu_size_to_slot(size: 0x3000)
static int pcpu_size_to_slot(int size)
{
	// size: 0x3000, pcpu_unit_size : 0x8000
	if (size == pcpu_unit_size)
		return pcpu_nr_slots - 1;

	// size: 0x3000
	return __pcpu_size_to_slot(size);
	// __pcpu_size_to_slot(0x3000): 11
}
```

## percpu.c::__pcpu_size_to_slot()

```
// __pcpu_size_to_slot(size: 0x8000)
// __pcpu_size_to_slot(size: 0x3000)
static int __pcpu_size_to_slot(int size)
{	
	// size: 0x8000
	// size: 0x3000
	int highbit = fls(size);	/* size is in bytes */
	// highbit = 16
	// highbit = 14

	// PCPU_SLOT_BASE_SHIFT: 5
	return max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);
	// max(13, 1) = 13
	// max(11, 1) = 11
}
```

## percpu.c::*pcpu_alloc()


list_for_each_entry(chunk, &pcpu_slot[slot], list) 를 정의를 풀어보자. 
		// #define list_for_each_entry(pos, head, member)				\
		// for (pos = list_first_entry(head, typeof(*pos), member);	\
	    //  &pos->member != (head);					\
	    //  pos = list_next_entry(pos, member))
		//
		// for (chunk = list_first_entry(&pcpu_slot[1], typeof(*chunk), list);
		//       &chunk->list != (&pcpu_slot[1]); chunk = list_next_entry(chunk, list))
		//
		// list_first_entry(&pcpu_slot[1])
		//
		// #define list_first_entry(ptr, type, member) \
		// list_entry((ptr)->next, type, member)
		//
		// list_first_entry(&pcpu_slot[1], typeof(*chunk), list):
		//     list_entry(&pcpu_slot[1]->next, typeof(*chunk), list)
		//
		// #define list_entry(ptr, type, member)		\
		// container_of(ptr, type, member)
		//
		// container_of(&pcpu_slot[1]->next, typeof(*chunk), list)

list_for_each_entry(chunk, &pcpu_slot[slot], list) {
// for (chunk = list_first_entry(&pcpu_slot[1], typeof(*chunk), list);
//       &chunk->list != (&pcpu_slot[1]); chunk = list_next_entry(chunk, list))
slot 1~10까지 list_for_each_entry는
&chunk->list != (&pcpu_slot[1])의 조건에 의해서
수행되지 않는다.
slot이 11일때 (dchunk로 값을 넣었음) 

```
	for (slot = pcpu_size_to_slot(size); slot < pcpu_nr_slots; slot++) {
	// pcpu_size_to_slot: size:16
	// slot: 1
		list_for_each_entry(chunk, &pcpu_slot[slot], list) {
		// for (chunk = list_first_entry(&pcpu_slot[1], typeof(*chunk), list);
		//       &chunk->list != (&pcpu_slot[1]); chunk = list_next_entry(chunk, list))

            // chunk: &pcpu_slot[11]

            // size: 16, chunk->contig_hint: &pcpu_slot[11]->contig_hint : 0x3000
			// 
			if (size > chunk->contig_hint)
				continue;

			new_alloc = pcpu_need_to_extend(chunk);
```
## pcpu_need_to_extend()
```
static int pcpu_need_to_extend(struct pcpu_chunk *chunk)
{
	int new_alloc;

    // chunk->map_alloc : dchunk->map_alloc: 128
	// dchunk->map_used : 2
    if (chunk->map_alloc >= chunk->map_used + 2)
	// trun 로 return 0을 함. 
		return 0;

	new_alloc = PCPU_DFL_MAP_ALLOC;
	while (new_alloc < chunk->map_used + 2)
		new_alloc *= 2;

	return new_alloc;
}
```

## percpu.c::*pcpu_alloc()
```
            // new_alloc: 0
            if (new_alloc) {
				spin_unlock_irqrestore(&pcpu_lock, flags);
				if (pcpu_extend_area_map(chunk,
							 new_alloc) < 0) {
					err = "failed to extend area map";
					goto fail_unlock_mutex;
				}
				spin_lock_irqsave(&pcpu_lock, flags);
				/*
				 * pcpu_lock has been dropped, need to
				 * restart cpu_slot list walking.
				 */
				goto restart;
			}
			// chunk: & 
			off = pcpu_alloc_area(chunk, size, align);
			if (off >= 0)
				goto area_found;
		}
	}
```

## pcpu_alloc_area()
```
static int pcpu_alloc_area(struct pcpu_chunk *chunk, int size, int align)
{
	int oslot = pcpu_chunk_slot(chunk);
	int max_contig = 0;
	int i, off;

    // chunk->map_used: dchunk->map_used:4, chunk->map[0]: dchunk->map_used[0]
	// -(__per_cpu 실제 할당한 size + 0x2000) 
	for (i = 0, off = 0; i < chunk->map_used; off += abs(chunk->map[i++])) {
	    // [loop 1] i: 0, off: 0, chunk->map_used: dchunk->map_used: 2
	    // [loop 2] i: 0, off: 0, chunk->map_used: dchunk->map_used: 2
		bool is_last = i + 1 == chunk->map_used;
		// [loop 1] is_last: 0
		// [loop 2] is_last: 1
		int head, tail;

		/* extra for alignment requirement */
		// [loop 1] off: 0, aligh: 8
		// [loop 2] off: __per_cpu에 실제 할당한 size + 0x2000, aligh: 8 		
		head = ALIGN(off, align) - off;
		// [loop 1] head: 0
		// [loop 2] head: 0

        // [loop 1] i: 0, head: 0
        // [loop 2] i: 0, head: 0 		
		BUG_ON(i == 0 && head != 0);

        // [loop 1] i: 0, chunk->map[0]: -(per_cpu 실제 할당항 size + 0x2000)
        // [loop 2] i: 1, chunk->map[1]: 0x3000
		if (chunk->map[i] < 0)
			continue;
		if (chunk->map[i] < head + size) {
			max_contig = max(chunk->map[i], max_contig);
			continue;
		}
		// [loop 2] i: 1, chunk->map[0]: 0x3000, head: 0, size: 16
		if (head && (head < sizeof(int) || chunk->map[i - 1] > 0)) {
			if (chunk->map[i - 1] > 0)
				chunk->map[i - 1] += head;
			else {
				chunk->map[i - 1] -= head;
				chunk->free_size -= head;
			}
			chunk->map[i] -= head;
			off += head;
			head = 0;
		}

		/* if tail is small, just keep it around */
		// [loop 2] i:1, chunk->map[1]: 0x3000, head: 0, size: 16
		tail = chunk->map[i] - head - size;
		// [loop 2] tail: 0x2ff0

        // tail: 0x2ff0 , sizeof(int): 4
		if (tail < sizeof(int))
			tail = 0;

		/* split if warranted */
		// head: 0, tail: 0x2ff0
// 2014/05/31 종료
```

## git log

```
		if (head || tail) {
			pcpu_split_block(chunk, i, head, tail);
			if (head) {
				i++;
				off += head;
				max_contig = max(chunk->map[i - 1], max_contig);
			}
			if (tail)
				max_contig = max(chunk->map[i + 1], max_contig);
		}

		/* update hint and mark allocated */
		if (is_last)
			chunk->contig_hint = max_contig; /* fully scanned */
		else
			chunk->contig_hint = max(chunk->contig_hint,
						 max_contig);

		chunk->free_size -= chunk->map[i];
		chunk->map[i] = -chunk->map[i];

		pcpu_chunk_relocate(chunk, oslot);
		return off;
	}
```


## slub.c::__kmem_cache_create()
```
// s: &boot_kmem_cache_node, flags: SLAB_HWCACHE_ALIGN: 0x00002000UL
int __kmem_cache_create(struct kmem_cache *s, unsigned long flags)
{
	int err;

	// s: &boot_kmem_cache_node, flags: SLAB_HWCACHE_ALIGN: 0x00002000UL
	err = kmem_cache_open(s, flags);
```

## git log
Updating c43e50c..ce36365
Fast-forward
 include/asm-generic/atomic-long.h |   1 +
 include/linux/compiler.h          |   1 +
 include/linux/kernel.h            |   1 +
 include/linux/list.h              |  10 ++
 include/linux/mm.h                |   1 +
 include/linux/page-flags.h        |  10 ++
 include/linux/percpu.h            |   2 +
 include/linux/poison.h            |   1 +
 include/linux/slab.h              |  10 +-
 include/linux/slub_def.h          |   7 +-
 mm/highmem.c                      |   2 +
 mm/percpu.c                       |  83 ++++++++++++-
 mm/slab.h                         |   4 +-
 mm/slub.c                         | 255 +++++++++++++++++++++++++++++++++++++-
 14 files changed, 378 insertions(+), 10 deletions(-)
