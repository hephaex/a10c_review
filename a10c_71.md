# ARM10C 71주차 후기
##### 일시 : 2014.09.27 (71주차)
##### 모임명 : NAVER개발자커뮤니티지원_IAMROOT.ORG_10차ARM-C
##### 장소 : 토즈 타워점
##### 장소지원 : NAVER 개발자 커뮤니티 지원 프로그램
##### 참여인원 :  3명

## 스터디 진도 : 
 - rcu_init()

## main.c::start_kernel()
* rcu_init()을 분석합니다.
* start_kernel()부터 rcu_init() 전에 어떤 일을 했는지 간추려서 보겠습니다.
 
```c
asmlinkage void __init start_kernel(void)
{
	char * command_line;
	extern const struct kernel_param __start___param[], __stop___param[];

...

	local_irq_disable();
	// IRQ를 disable한다.
	early_boot_irqs_disabled = true;

	boot_cpu_init()
	// 현재 cpu(core id)를 얻어서 cpu_XXX_bits[] 의 cpu를 셋한다.
	
	page_address_init();
	// 128개의 page_address_htable 배열을 초기화
	
	pr_notice("%s", linux_banner);
	// 배너:
	//	Linux version 2.6.37_DM385_IPNC_3.50.00
	// 	(a0875405@bangvideoapps01) (gcc version 4.5.3 20110311 
	// 	(prerelease) (GCC) ) #1 Fri Dec 21 17:27:08 IST 2012

    setup_arch(&command_line);
	// ARM 아키텍쳐에 맞게 초기화한다.

	setup_nr_cpu_ids();
	setup_per_cpu_areas();
	// pcpu 구조체를 만들어 줌 (mm/percpu.c)

	smp_prepare_boot_cpu();	/* arch-specific boot-cpu hooks */
	// boot cpu 0의 pcpu 영역의 base주소를 core register에 설정해줌

...
	pidhash_init();
	// pidhash의 크기를 16kB만큼 할당 받고 4096개의 hash list를 만듬

	vfs_caches_init_early();
	// Dentry cache, Inode-cache용 hash를 위한 메모리 공간을 각각 512kB, 256kB만큼 할당 받고,
	// 131072, 65536개 만큼 hash table을 각각 만듬

...
	mm_init();
	// buddy와 slab 을 활성화 하고 기존 할당 받은 bootmem 은 buddy,
	// pcpu 메모리, vmlist 는 slab으로 이관

	sched_init();
	// scheduler가 사용하는 자료 구조 초기화, idle_threads를 init_task로 세팅

	/*
	 * Disable preemption - early bootup scheduling is extremely
	 * fragile until we cpu_idle() for the first time.
	 */
	preempt_disable();
	// preempt count를 증가시켜 preemption 못하도록 막음

	// irqs_disabled(): 1
	if (WARN(!irqs_disabled(), "Interrupts were enabled *very* early, fixing it\n"))
		local_irq_disable();

	idr_init_cache();
	// integer ID management로 사용하는 idr_layer_cache에 kmem_cache#21 을 생성 및 초기화 후 할당

	rcu_init();	
```

* mm_init()에서 buddy와 slab을 활성화하고 sched_init()을 실행한다.
* sched_init()에서 rq를 초기화하고 // scheduler_running: 1
* integer ID management로 사용하는 idr_layer_cache에 kmem_cache#21 을 생성 및 초기화 후 할당
*

## tree.c::rcu_init()

```c
// ARM10C 20140920
void __init rcu_init(void)
{
	int cpu;

	rcu_bootup_announce();
	rcu_init_geometry();
	// rcu_init_geometry에서 한일:
	// jiffies_till_first_fqs: 1
	// jiffies_till_next_fqs: 1

// 2014/09/20 종료

	rcu_init_one(&rcu_bh_state, &rcu_bh_data);
	rcu_init_one(&rcu_sched_state, &rcu_sched_data);
	__rcu_init_preempt();
	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);

	/*
	 * We don't need protection against CPU-hotplug here because
	 * this is called early in boot, before either interrupts
	 * or the scheduler are operational.
	 */
	cpu_notifier(rcu_cpu_notify, 0);
	pm_notifier(rcu_pm_notify, 0);
	for_each_online_cpu(cpu)
		rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
}
```

### rcu_bootup_announce();
### rcu_init_geometry();
	// rcu_init_geometry에서 한일:
	// jiffies_till_first_fqs: 1
	// jiffies_till_next_fqs: 1

### tree.c::rcu_init_one(&rcu_bh_state, &rcu_bh_data);
* call:	rcu_init_one(&rcu_bh_state, &rcu_bh_data);

```c
static void __init rcu_init_one(struct rcu_state *rsp,
		struct rcu_data __percpu *rda)
{
	static char *buf[] = { "rcu_node_0",
			       "rcu_node_1",
			       "rcu_node_2",
			       "rcu_node_3" };  /* Match MAX_RCU_LVLS */
	static char *fqs[] = { "rcu_node_fqs_0",
			       "rcu_node_fqs_1",
			       "rcu_node_fqs_2",
			       "rcu_node_fqs_3" };  /* Match MAX_RCU_LVLS */
	int cpustride = 1;
	int i;
	int j;
	struct rcu_node *rnp;

    // MAX_RCU_LVLS: 4, ARRAY_SIZE(buf): 4
	BUILD_BUG_ON(MAX_RCU_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */

	/* Silence gcc 4.8 warning about array index out of range. */
	// rcu_num_lvls: 1, RCU_NUM_LVLS: 1
	if (rcu_num_lvls > RCU_NUM_LVLS)
		panic("rcu_init_one: rcu_num_lvls overflow");

	/* Initialize the level-tracking arrays. */

    /*
	* #  define RCU_NUM_LVLS	      1
	* #  define NUM_RCU_LVL_0	      1
	* #  define NUM_RCU_LVL_1	      (NR_CPUS) : 4
	* #  define NUM_RCU_LVL_2	      0
	* #  define NUM_RCU_LVL_3	      0
	*/
	
	for (i = 0; i < rcu_num_lvls; i++)
	    // rsp->levelcnt[i]: &rcu_bh_state->levelcnt[i]
		rsp->levelcnt[i] = num_rcu_lvl[i];
		// rsp->levelcnt[0]: &rcu_bh_state->levelcnt[0]: 1
		
	for (i = 1; i < rcu_num_lvls; i++)
	// rcu_num_lvls 가 1이상일때 각노드의 level을 헤아려서 계산(cnt)한다. 
		rsp->level[i] = rsp->level[i - 1] + rsp->levelcnt[i - 1];
		
	rcu_init_levelspread(rsp);
	/*  static void __init rcu_init_levelspread(struct rcu_state *rsp)
	*   {
	*   int ccur;
	*   int cprv;
	*   int i;
    *
	*   cprv = nr_cpu_ids;
	*   // cprv: nr_cpu_ids: 4
	*   for (i = rcu_num_lvls - 1; i >= 0; i--) {
	*	    ccur = rsp->levelcnt[i];
	*	    rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
	        // rsp->levelspead[0]: // (&rcp_bh_state)->levelspread[0]: 4
	*	    cprv = ccur;
	        
	*	    }
	*   }
	*/
	/* Initialize the elements themselves, starting from the leaves. */

	for (i = rcu_num_lvls - 1; i >= 0; i--) {
		cpustride *= rsp->levelspread[i];
		// cpustirde: 1 * 4 : 4
		rnp = rsp->level[i];
		// rnp의 첫번째 노드 값
		for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
			raw_spin_lock_init(&rnp->lock);
			lockdep_set_class_and_name(&rnp->lock,
						   &rcu_node_class[i], buf[i]);
			   // rcu_node_class[0], buf[0]: rcu_node_0,
			   // null function
			// rnp: (&rcu_bh_state)->level[0]->fqslock
			raw_spin_lock_init(&rnp->fqslock);
			lockdep_set_class_and_name(&rnp->fqslock,
						   &rcu_fqs_class[i], fqs[i]);
			// rnp->gpnum: ((&rcu_bh_stat)->level[0])->gpnum: rsp->gpnum 
			rnp->gpnum = rsp->gpnum;
			// ((&rcu_bh_stat)->level[0])->gpnum: 0UL - 300UL : 0xfffffed4
			rnp->completed = rsp->completed;
			// ((&rcu_bh_stat)->level[0])->completed: 0UL - 300UL : 0xfffffed4
			rnp->qsmask = 0;
			rnp->qsmaskinit = 0;

            // rnp
			rnp->grplo = j * cpustride;
			rnp->grphi = (j + 1) * cpustride - 1;
			if (rnp->grphi >= NR_CPUS)
				rnp->grphi = NR_CPUS - 1;
			if (i == 0) {
				rnp->grpnum = 0;
				rnp->grpmask = 0;
				rnp->parent = NULL;
			} else {
				rnp->grpnum = j % rsp->levelspread[i - 1];
				rnp->grpmask = 1UL << rnp->grpnum;
				rnp->parent = rsp->level[i - 1] +
					      j / rsp->levelspread[i - 1];
			}
			rnp->level = i;
			INIT_LIST_HEAD(&rnp->blkd_tasks);
			rcu_init_one_nocb(rnp);
		}
	}

    // rsp->rds: (&rcu_bh_stat)->rda, rda: &rcu_bh_data
	rsp->rda = rda;
	
	init_waitqueue_head(&rsp->gp_wq);
	// wait.c::init_waitqueue_head()

    // &rsp->wakeup_work: 
	init_irq_work(&rsp->wakeup_work, rsp_wakeup);
	
	rnp = rsp->level[rcu_num_lvls - 1];
	for_each_possible_cpu(i) {
		while (i > rnp->grphi)
			rnp++;
		per_cpu_ptr(rsp->rda, i)->mynode = rnp;
		// rsp->rda: (&rcu_bh_state)->rda: &rcu_bh_data,r
		rcu_boot_init_percpu_data(i, rsp);
	}
	list_add(&rsp->flavors, &rcu_struct_flavors);
}
```

* tree.c:: RCU_STATE_INITIALIZER(sname, sabbr, cr)
```c
#define RCU_STATE_INITIALIZER(sname, sabbr, cr) \
static char sname##_varname[] = #sname; \
static const char *tp_##sname##_varname __used __tracepoint_string = sname##_varname; \
struct rcu_state sname##_state = { \
	.level = { &sname##_state.node[0] }, \
	.call = cr, \
	.fqs_state = RCU_GP_IDLE, \
	.gpnum = 0UL - 300UL, \
	.completed = 0UL - 300UL, \
	.orphan_lock = __RAW_SPIN_LOCK_UNLOCKED(&sname##_state.orphan_lock), \
	.orphan_nxttail = &sname##_state.orphan_nxtlist, \
	.orphan_donetail = &sname##_state.orphan_donelist, \
	.barrier_mutex = __MUTEX_INITIALIZER(sname##_state.barrier_mutex), \
	.onoff_mutex = __MUTEX_INITIALIZER(sname##_state.onoff_mutex), \
	.name = sname##_varname, \
	.abbr = sabbr, \
}; \
DEFINE_PER_CPU(struct rcu_data, sname##_data)

RCU_STATE_INITIALIZER(rcu_sched, 's', call_rcu_sched);
RCU_STATE_INITIALIZER(rcu_bh, 'b', call_rcu_bh);
```

### wait.c::init_waitqueue_head()

```c
#define init_waitqueue_head(q)				\
	do {						\
		static struct lock_class_key __key;	\
							\
		__init_waitqueue_head((q), #q, &__key);	\
	} while (0)
```

## irq_work.h::init_irq_work()
* &work

```c
static inline
void init_irq_work(struct irq_work *work, void (*func)(struct irq_work *))
{
	work->flags = 0;
	work->func = func;
}
```

### tree.c::rcu_boot_init_percpu_data()

```c
static void __init
rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
{
	unsigned long flags;
	struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
	
	struct rcu_node *rnp = rcu_get_root(rsp);
	// rn

	/* Set up local state, ensuring consistent view of global state. */
	raw_spin_lock_irqsave(&rnp->lock, flags);
	
	rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
	// rdp->grpmask: [pcp0] &rcu_bh_data->grpmask

    // rdp: [pcp0] &rcu_bh_data
	init_callback_list(rdp);

    // rdp->qlen_lazy: [pcp0] (&rcu_bh_data)->qlen_lazy
	rdp->qlen_lazy = 0;
    // rdp->qlen_lazy: [pcp0] (&rcu_bh_data)->qlen_lazy:0
	
	ACCESS_ONCE(rdp->qlen) = 0;

    // cpu:0 [pcp0] &per_cpu((&rcu_bh_data)->dynticks, 0 )
	rdp->dynticks = &per_cpu(rcu_dynticks, cpu);


	WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
	WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);

    rdp->cpu = cpu;
	// [pcp0] (&rcu_bh_data)->cpu: cpu: 0
	
	rdp->rsp = rsp;
	// [pcp0] (&rcu_bh_data)->rsp: rsp: &rcu_bh_state
	
	rcu_boot_init_nocb_percpu_data(rdp);
	// null function
	
	raw_spin_unlock_irqrestore(&rnp->lock, flags);
}
```
